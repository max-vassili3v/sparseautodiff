include("sparseautodiff.jl")
include("gradientdescent.jl")
using .SparseAutoDiff, LinearAlgebra, Statistics, Plots

elemwise_relu(ğ›,p) = relu.(ğ› .+ p)

#Entire process of obtaining âˆ‡f(x) for gradient descent, as well as the loss.
function differentiate_model(ğ›,x)
    l = length(ğ›)
    #relu pullback
    y, t = pullback_diagonal(elemwise_relu, ğ›; ğ© = x)
    #summation pullback
    y2, t2 = sum(y), ones(l)
    #loss pullback
    y3, t3 = (y2 - tan(x))^2, 2*(y2-tan(x))

    t(t2*t3), y3
end

#Simple gradient descent, 'batching' over the interval [0,1]
function gradient_descent(n,N,Î³)
    ğ± = rand(n)
    for i=1:N
        Î´ = []
        loss = []
        for j in LinRange(0,1,10)
            #Find losses and gradients at individual points
            d,l = differentiate_model(ğ±,j)
            push!(Î´,d)
            push!(loss,l)
        end
        #Compute and apply means.
        Î´_final, loss_final = mean(Î´), mean(loss)
        ğ± -= Î³*Î´_final
        println("iteration $i loss $loss_final")
    end
    ğ±
end

ğ› = gradient_descent(4,200,0.1)
x = LinRange(0,1,10)
y = [sum(elemwise_relu(ğ›,j)) for j in x]

plot(x,tan.(x))
plot!(x,y)